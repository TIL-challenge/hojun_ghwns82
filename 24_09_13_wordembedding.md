### 1. **모델 구조**

- **Input Layer**: 단어가 원-핫 인코딩(one-hot encoding) 방식으로 표현됩니다. 예를 들어, "study"라는 단어가 입력되면, 단어가 어휘(vocabulary)에서 차지하는 위치에 따라 [0, 1, 0]으로 인코딩됩니다.
- **Hidden Layer**: 가중치 행렬 **W₁**을 거치며, 이 행렬의 각 열은 어휘 내의 단어에 대응됩니다. 이때, 입력된 단어는 **W₁**의 대응되는 열을 활성화하게 됩니다.
- **Output Layer**: 두 번째 가중치 행렬 **W₂**를 통과하며, 각 단어의 연관성(내적)이 계산됩니다. 이 과정은 예측된 단어를 결정하는 단계입니다.

### 2. **구체적 예시**

- 입력 문장은 "I study math"입니다.
- 어휘(vocabulary)는 {I, study, math}로 이루어져 있으며, 각 단어는 원-핫 벡터로 표현됩니다.
    - 예를 들어, "study"가 입력되면 [0, 1, 0]이 입력 벡터로 사용됩니다.
- **입력**: "study" [0, 1, 0]이 들어가면, 해당 단어에 대응되는 벡터가 **W₁**에서 활성화됩니다.
- **출력**: 모델은 "math"를 예측하고, 결과적으로 [0, 0, 1]이 출력됩니다.

### 3. **가중치 행렬 (W₁, W₂)**

- **W₁**: 각 열이 단어 벡터에 해당하는 가중치 행렬입니다.
- **W₂**: 각 행이 단어 벡터에 해당하는 가중치 행렬입니다.
    - 예시에서는 "study"에 해당하는 벡터와 "math"에 해당하는 벡터 간의 내적이 커야 하므로, 두 벡터 간의 내적 값이 중요합니다.

### 4. **소프트맥스 (Softmax)**

- **Softmax** 함수는 출력된 값들이 확률로 변환되도록 합니다. 여기서 높은 내적 값을 가진 단어가 출력될 가능성이 높아집니다. 예를 들어, "study" 벡터가 입력되었을 때, "math" 벡터와의 내적 값이 높으면 "math"가 출력됩니다.

### 5. **핵심 포인트**

- **단어 벡터**는 모델이 학습하는 과정에서 단어 간의 연관성을 학습하며, 유사한 문맥에서 자주 등장하는 단어들은 유사한 벡터를 가지게 됩니다.
- **W₁의 열**과 **W₂의 행**은 각 단어의 벡터를 나타내며, 이 벡터 간의 내적을 통해 단어의 연관성을 파악합니다.

### 1. **시각화 구조**

- **중앙의 그래프**: 중간에 있는 노드는 입력된 단어("eat")를 나타내고, 양쪽의 단어들과 연결된 선들은 연관성을 나타냅니다. 입력 단어가 출력될 가능성이 높은 단어들과의 내적 값에 따라 연결이 형성됩니다.
    - 예를 들어, "eat"과 관련된 단어로 **apple**, **orange**, **rice** 등이 연결되어 있으며, 이 단어들과의 내적 값이 높다는 것을 의미합니다.

### 2. **가중치 행렬**

- **좌측 하단 (Weight Matrices)**:
    - **Input Vector**: 입력 단어 벡터 ("eat")가 **W₁** 행렬에서 어떻게 표현되는지 나타냅니다.
    - **Output Vector**: 출력 후보 단어들이 **W₂** 행렬에서 표현되는 방식입니다.
    - 이 두 벡터 간의 내적 값이 높을수록, 해당 단어가 예측될 확률이 높아집니다. 여기서는 "eat"과 "apple", "orange", "rice"의 내적 값이 높은 것을 확인할 수 있습니다.

### 3. **벡터 관계 (우측 하단 그래프)**

- **벡터 공간**에서 단어들이 시각화된 모습입니다. **입력 벡터**와 **출력 벡터**가 가까운 위치에 있으면, 그 단어들이 서로 연관성이 높다는 의미입니다. 여기서는 "eat"과 "apple", "orange", "rice"가 같은 클러스터에 묶여 있어 연관성이 높음을 나타냅니다.

### 4. **설명 내용**

- **'eat'의 벡터 표현**이 **W₁**에서의 **apple**, **orange**, **rice** 벡터와 유사한 패턴을 가지므로, 모델은 "eat"이 입력되었을 때 **apple**, **orange**, **rice**를 출력으로 예측할 수 있습니다.
- **내적 값**이 높다는 것은, 벡터 공간 상에서 두 단어가 가까이 위치해 있다는 의미입니다. 따라서 문맥상 연관성이 높다는 것을 나타냅니다.

### 5. **활용**

이 도구는 단어 벡터 간의 관계를 분석하는 데 유용하며, 특히 자연어 처리(NLP)에서 단어들이 어떻게 벡터 공간에서 학습되고, 이 벡터들이 어떻게 상호작용하는지를 시각적으로 이해하는 데 도움이 됩니다.

### 핵심 내용

1. **단어 벡터 (Word Vector)**:
    - 단어 벡터는 단어 간의 관계를 나타내며, 벡터 공간에서 단어들이 어떻게 위치하는지에 따라 단어들 간의 의미적 관계를 알 수 있습니다.
    - 벡터 공간 상에서의 방향과 크기가 단어들의 의미적 차이를 나타냅니다.
2. **벡터 관계 예시**:
    - 이미지의 첫 번째 그래프에서, "man"과 "woman"의 관계는 "king"과 "queen"의 관계와 유사하게 표현됩니다. 이는 두 관계가 벡터 공간에서 비슷한 방향과 크기를 가지고 있음을 의미합니다.
    - 예를 들어, "queen"에서 "king"을 빼는 벡터는 "woman"에서 "man"을 빼는 벡터와 동일한 방향과 크기를 갖습니다.
        - 수식으로 표현하면:
        \[
        vec[queen] - vec[king] = vec[woman] - vec[man]
        \]
        - 이는 단어 간의 관계가 유사할 경우, 벡터 연산에서도 그 관계가 유지됨을 보여줍니다.
3. **벡터 유사성**:
    - 두 번째 그래프에서는 "king"과 "queen"의 관계가 "kings"와 "queens"의 관계와 유사하게 나타나 있습니다.
    - 단수와 복수의 관계 역시 벡터 공간에서 동일한 패턴을 따릅니다.
### 응용

- 이러한 벡터 관계는 **단어 임베딩**에서 중요한 역할을 하며, 단어 간의 유사성과 관계를 수학적으로 표현할 수 있게 합니다.
- 이를 통해 **단어 유추(word analogy)** 문제를 풀 수 있으며, 예를 들어 "king:queen = man:woman"과 같은 관계를 자동으로 추론할 수 있습니다.

이처럼 단어 벡터는 단순한 단어 간의 유사성뿐만 아니라, 의미적 관계까지 벡터 공간에서 수학적으로 표현할 수 있는 도구입니다.

### Word2Vec이 NLP에서 성능을 향상시키는 주요 분야

1. **Word Similarity (단어 유사도)**:
    - Word2Vec을 사용하면 단어들 간의 유사도를 벡터 거리로 계산할 수 있습니다. 의미적으로 유사한 단어들은 벡터 공간에서 가까운 거리에 위치하게 됩니다.
2. **Machine Translation (기계 번역)**:
    - 단어 간의 의미적 관계를 벡터로 표현하기 때문에, 언어 간의 유사한 패턴을 학습하여 기계 번역에서 더 나은 성능을 발휘할 수 있습니다.
3. **Part-of-speech (PoS) Tagging (품사 태깅)**:
    - 단어 벡터를 활용하여 문장에서 각 단어의 품사를 예측하는 데 도움을 줍니다. Word2Vec을 사용하면 단어 간의 문법적 관계를 학습할 수 있습니다.
4. **Named Entity Recognition (NER) (개체명 인식)**:
    - NER은 문장에서 인물, 장소, 기관명 등의 고유 명사를 인식하는 작업입니다. Word2Vec은 단어들의 의미적 유사성을 통해 NER 성능을 향상시킬 수 있습니다.
5. **Sentiment Analysis (감정 분석)**:
    - 텍스트에서 긍정적인지 부정적인지를 분류하는 감정 분석에서도 Word2Vec을 사용하여 문장 내 단어 간의 의미적 관계를 학습함으로써 감정 분류의 정확도를 높일 수 있습니다.
6. **Clustering (군집화)**:
    - 의미적으로 유사한 단어들을 같은 클러스터로 묶는 데 도움이 됩니다. Word2Vec은 단어를 벡터로 변환하여 유사한 단어들을 그룹화할 수 있습니다.
7. **Semantic Lexicon Building (의미론적 사전 구축)**:
    - 단어들의 벡터 표현을 사용하여 의미적 관계에 기반한 사전을 구축할 수 있습니다. 이를 통해 단어 간의 의미적 연관성을 쉽게 파악할 수 있습니다.

### 요약

Word2Vec은 다양한 자연어 처리 작업에서 단어 간의 의미적 유사성과 관계를 학습함으로써 성능을 크게 개선시킵니다. 이는 단어의 벡터 표현을 활용하여 보다 정교한 언어 처리 작업이 가능하게 하는 중요한 기법입니다.

### 주요 내용

1. **영어 단어 벡터 (왼쪽)**:
    - 숫자: "one", "two", "three", "four", "five"
    - 동물: "horse", "cow", "pig", "dog", "cat"
    - 이 단어들은 벡터 공간에서 특정 위치에 분포되어 있으며, 유사한 의미를 가진 단어들이 가까운 위치에 있습니다.
2. **스페인어 단어 벡터 (오른쪽)**:
    - 숫자: "uno", "dos", "tres", "cuatro", "cinco"
    - 동물: "caballo" (horse), "vaca" (cow), "cerdo" (pig), "perro" (dog), "gato" (cat)
    - 마찬가지로 스페인어에서도 유사한 개념을 가진 단어들이 비슷한 위치에 분포되어 있습니다.
3. **비교와 해석**:
    - 영어와 스페인어 단어들이 벡터 공간에서 매우 유사한 구조를 보이고 있음을 알 수 있습니다. 예를 들어, 영어의 "dog"와 스페인어의 "perro", 영어의 "three"와 스페인어의 "tres"가 각각 유사한 위치에 있습니다.
    - 이는 벡터 공간 내에서 두 언어 간의 개념적 유사성을 학습할 수 있음을 의미하며, 이 벡터 간의 선형 변환(linear mapping)을 통해 한 언어에서 다른 언어로 번역이 가능하다는 것을 시사합니다.
