### **Word Embedding(Word2Vec)**

- Word2Vec
    - **단어 벡터 표현을 학습하는 알고리즘**: 주어진 단어의 인접 단어(맥락)로부터 단어의 벡터 표현을 학습합니다.
    - **가정**: 유사한 맥락에서 등장하는 단어들은 유사한 의미를 가질 것이라고 가정합니다.
    - **예시**:
        - "The cat purrs."
        - "This cat hunts mice."
    
    **word2vec**의 기본 아이디어 : 어떻게 단어들 간의 의미적 유사성을 벡터 공간에서 학습
    
    - J.R. Firth의 유명한 말, **"You shall know a word by the company it keeps"** (1957년): 단어는 그 주변 단어들에 의해 그 의미가 정해진다는 것을 의미합니다.
    - 예시로, "cat"이라는 단어가 주어졌을 때, **P(w|cat)**, 즉 주어진 "cat"이라는 단어 근처에서 단어 **w**가 나올 확률을 계산하려고 합니다.
    - 그래프는 "cat" 근처에 나올 수 있는 단어들(w)의 확률 분포를 보여줍니다:
        - 예를 들어, "meow"는 높은 확률로 "cat" 근처에 나타날 수 있고, "fleas"나 "pet"도 비교적 높은 확률을 가집니다.
        - 반면에 "potato", "Paris", "baguette"와 같은 단어들은 낮은 확률로 나타납니다.
    - **분포 가설 (Distributional Hypothesis)**: "cat"의 의미는 확률 분포 **P(w|cat)**를 통해 포착될 수 있습니다. 즉, "cat"이라는 단어는 그 주변에서 나타나는 단어들의 분포를 통해 그 의미를 알 수 있다는 것입니다.
