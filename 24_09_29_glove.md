GloVe는 말뭉치(corpus)에서 단어들의 동시 출현 통계를 바탕으로 단어 벡터를 학습하는 방법입니다. 내용을 자세히 설명하자면:

1. **GloVe 개요**:
    - 동일한 단어 쌍에 대해 반복적으로 훈련하지 않고, 말뭉치에서 단어들이 함께 출현하는 빈도를 요약하는 **동시 출현 행렬**을 계산합니다.
    - 이 동시 출현 행렬을 기반으로 행렬 분해를 수행하여 단어 벡터를 생성합니다.
2. **손실 함수 $J(\theta)$**:
    - 손실 함수는 모든 단어 쌍 i,j에 대해 합을 구하며, 이때 동시 출현 확률 Pij에 기반한 가중치 함수 $f(P_{ij})$를 적용하여, 단어 벡터의 내적 $u_{i}^Tv_j$와 동시 출현 확률의 로그  $logP_{ij}$간의 차이를 측정합니다.
    - 함수 $f$는 단어 쌍마다 손실 함수에서 적용되는 가중치를 조정합니다.
3. **장점**:
    - 빠른 학습.
    - 작은 말뭉치에서도 잘 작동함.
4. **그래프**: 오른쪽에 있는 그래프는 함수 f를 나타내며, 특정 값 이상에서는 상한선에 도달하는 것으로 보입니다. 이는 단어 쌍의 동시 출현 빈도가 일정 수준을 넘으면 가중치가 감소하는 방식을 나타내는 것 같습니다.

정리하자면, GloVe는 동시 출현 행렬을 분해하여 단어 벡터를 계산하는 방법으로, 효율적인 학습이 가능하고 작은 데이터셋에서도 좋은 성능을 발휘합니다.

- 사전에 이미 대규모 데이터로 학습된 모델이 오픈소스로 공개되어 있습니다. 해당 모델은 위키피디아 데이터를 기반으로 하여 6B token만큼 학습 되었으며, 중복 제거 시에도 단어의 개수가 무려 40만개(400k)에 달합니다.
- 학습된 모델을 나타낼 때 뒤에 붙는 "uncased"는 대문자 소문자를 구분하지 않는다는 의미이며, 반대로 "cased"는 대소문자를 구분한다는 의미입니다. 예를 들어 Cat과 cat이 uncased에서는 같은 토큰으로 취급되지만, cased에서는 다른 토큰으로 취급됩니다.
- Glove 깃헙 주소 : https://github.com/stanfordnlp/GloVe (기존에 학습된 워드 임베딩도 다운로드 받아 사용할 수 있습니다.)
